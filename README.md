# EECS289_Final_Project_T
Week 10 of EECS16ML covers the topic of Natural Language Processing. In order to transform text into fixed-length and numerical form that computer algorithms favor, and at the same time retaining the rich meanings included in the context, several word embedding techniques have been developed to meet the challenge. We hope students to understand both types of count-based and predictive methods along with several popular algorithms in our repository. 

## Learning Objective
  Students will learn the following word embedding algorithms as well as their applications in real machine learning tasks. Students will understand the mechanisms behind those algorithms by looking at the note/slide and/or implementing these algoirthm by themselves. Students will also be introduced efficient libraries to directly use thes algorithms, including Pandas, Skit-Learn and Gensim. In addition to building word embedding models, students will also learn how to prepare the training data with those techniques and apply to the real machine learning task, movie sentiment analysis on IMDB dataset, in this assignment.
  
  * Count-based
    * Bag of Words
    * TFIDF
    * Co-occurance matrix
  
  * Predictive
    * Word2Vec
    * GloVe (need Co-occuranc matrix)
    * Bert

## Pre-requisites/Prior knowledge 
  * Week 1: 
    * NumPy
    * Pandas 
  * Week 2: 
    * Spliting training/test data
    * Scikit-Learn for classification task 
    * Implementing Stochastic Gradient Descent (for Word2Vec, GloVe and Bert)
  * Week 3: 
    * SVD/PCA for dimension reduction
