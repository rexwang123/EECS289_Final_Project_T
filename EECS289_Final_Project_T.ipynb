{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1w6S2tf_1r4"
   },
   "source": [
    "# **EECS 16ML Assignment: Traditional/Contextual Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE0fNvxdAfxt"
   },
   "source": [
    "Before you start, make sure you review the lecture slide and notes about word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGllTplLAP5V",
    "outputId": "019b20ca-282a-4748-cdcd-cf2f9a8c10e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# if error appears, uncomment the next line\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM4Mwv-cXoEW"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(review):\n",
    "    # remove HTML contents.\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    review = soup.get_text()\n",
    "\n",
    "    # remove everything except lower/upper case letters using Regular Expressions.\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "\n",
    "    # bring everything into lowercase.\n",
    "    review = review.lower()\n",
    "\n",
    "    review = review.split()\n",
    "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "vkPT8RaPHKT2",
    "outputId": "f87315fa-dacc-4b4d-d1ee-475c387e5fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset and visualize the first ten data points\n",
    "path_data = '/content/IMDB Dataset.csv'\n",
    "imdb_data=pd.read_csv(path_data)\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4S1utlqf29N"
   },
   "source": [
    "Before vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXA3S5ocKsPn",
    "outputId": "90cd774c-cca3-4918-daf1-6884ee334f8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [25:05<00:00, 33.21it/s]\n"
     ]
    }
   ],
   "source": [
    "X_raw = imdb_data['review']\n",
    "y = imdb_data['sentiment']\n",
    "# Clean the raw data\n",
    "tqdm.pandas()\n",
    "X = X_raw.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0Y5_pSZcVbj",
    "outputId": "87b85e05-ee71-4004-a655-b2f6d9cfd57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [one, review, mention, watch, oz, episod, hook...\n",
       "1        [wonder, littl, product, film, techniqu, unass...\n",
       "2        [thought, wonder, way, spend, time, hot, summe...\n",
       "3        [basic, famili, littl, boy, jake, think, zombi...\n",
       "4        [petter, mattei, love, time, money, visual, st...\n",
       "                               ...                        \n",
       "49995    [thought, movi, right, good, job, creativ, ori...\n",
       "49996    [bad, plot, bad, dialogu, bad, act, idiot, dir...\n",
       "49997    [cathol, taught, parochi, elementari, school, ...\n",
       "49998    [go, disagre, previou, comment, side, maltin, ...\n",
       "49999    [one, expect, star, trek, movi, high, art, fan...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGIb63I8EWq4"
   },
   "source": [
    "### **Part 1: Word-Count Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHO8G9pI_mL5"
   },
   "source": [
    "**Explain bag of words approach**\n",
    "\n",
    "Define `vectorize` and `vectorizeDF` below to implement the Bag of Words approach, then apply `vectorizeDF` on the first 5,000 reviews.\n",
    "\n",
    "*hint: use `.value_counts()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt_a--1L-ZPZ"
   },
   "outputs": [],
   "source": [
    "# For now, use only first 5000 reviews\n",
    "reviews_5000 = X[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF-BW6TOQ9mm"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a vector by counting the occurences of every word in the list\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: vectorOfWords - a Pandas series containing occurences of every word in the input list\n",
    "def vectorize(bagOfWords):\n",
    "    #TODO \n",
    "    return pd.Series(bagOfWords).value_counts()\n",
    "\n",
    "# This function applies vectorize() and vectorize all lists of words included in a Pandas dataframe\n",
    "# Input: reviews - a Pandas dataframe containing all the reviews in *list of words* form\n",
    "# Output: X_vectorized - a Pandas dataframe containing vectorized form of all reviews\n",
    "def vectorizeDF(reviews):\n",
    "    #TODO\n",
    "    return reviews.progress_apply(vectorize).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtRFjFAj486_"
   },
   "outputs": [],
   "source": [
    "X_vectorized = vectorizeDF(reviews_5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgIbAsTn5RZa"
   },
   "source": [
    "Check if applying implemented `vectorize` and `vectorizeDF` to out lists of words produce correct length of vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egYy1gT-5TxS"
   },
   "outputs": [],
   "source": [
    "len(set(reviews_5000.explode())) == len(X_vectorized.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGfd9bw_RFhH"
   },
   "source": [
    "**explain n-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmspZgtjOw_Z"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a list of 2-grams\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: bagOf2grams - a list of 2-grams: [(word1, word2), (word2, word3) ... ]\n",
    "def bigram(bagOfWords):\n",
    "    #TODO\n",
    "    return [(bagOfWords[i], bagOfWords[i+1]) for i in range(len(bagOfWords)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahQylR3HO6VP"
   },
   "outputs": [],
   "source": [
    "reviews_5000.apply(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy3EFvb2RIuW"
   },
   "source": [
    "**explain nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dmjzOU0RLV6"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2jXICGbRSMh"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a list of 6-grams\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: bagOf6grams - a list of 6-grams: [(word1, word2, word3, word4, word5, word6) ... ]\n",
    "def sixgram(bagOfWords):\n",
    "    #TODO\n",
    "    return list(ngrams(bagOfWords, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKdEpgtJRyro"
   },
   "outputs": [],
   "source": [
    "reviews_5000.apply(sixgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZN2ZVPe5z0S"
   },
   "source": [
    "In the real world, implementing Bag of Words from scratch can be computation-heavy, especially on large dataset with longer texts. Fortunately, `Scikit-learn` has implemented it in an efficient way for us with `CountVectorizer`. Now using `CountVectorizer`, redefine `vectorizeDF` and apply it on the whole dataset. Feel free to play with the  parameter `ngram_range`, default (1, 1), for different ngrams. Note that `CountVectorizer` has a method `get_features_name()` that shows all the word tokens after vectorized if you want to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ip_3b_xm6QaY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Modify our review data into string sentence form\n",
    "reviews = X.progress_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuEibO6C9iV5"
   },
   "outputs": [],
   "source": [
    "# This function vectorize all texts included in a Pandas dataframe\n",
    "# Input: reviews - a list containing all the reviews in *string sentence* form\n",
    "# Output: X_vectorized - a numpy array containing vectorized form of all reviews\n",
    "def vectorizeDF(reviews):\n",
    "    #TODO\n",
    "    vectorizer = CountVectorizer(ngram_range = (1, 2))\n",
    "    X_vectorized = vectorizer.fit_transform(reviews)\n",
    "    return X_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEymhCHs_XxY"
   },
   "outputs": [],
   "source": [
    "X_vectorized = vectorizeDF(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZcBKzBi5WHl"
   },
   "source": [
    "Then, we use this vectorized data to train a classifier and report accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqc-WKKO5Ys8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_train)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSRpvE80EkPJ"
   },
   "source": [
    "### **Part 2: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAjiXPMXKKzO"
   },
   "outputs": [],
   "source": [
    "# This function computes the term frequency (TF), i.e. the number of times a word appears in a review divded \n",
    "# by the total number of words in the review.\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: tfDict - a dictionary containing the TF scores of all the words in a review\n",
    "def computeTF(bagOfWords):\n",
    "    # Frequency of each word in a review\n",
    "    wordDict = dict(Counter(bagOfWords))\n",
    "    tfDict = {}\n",
    "    N = len(bagOfWords)\n",
    "    # TODO: compute term frequency\n",
    "    # Hint: Don't forget to apply logarithm transformation \n",
    "    for word, count in wordDict.items():\n",
    "            tfDict[word] = np.log(1+count / float(N))\n",
    "\n",
    "    return tfDict\n",
    "\n",
    "# This function computes the inverse document frequency (IDF), i.e. the number of reviews divided by the number \n",
    "# of reviews that contain the word w. \n",
    "# Input: reviews - a list containing all the reviews\n",
    "# Output: idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "def computeIDF(reviews):\n",
    "    N = len(reviews)\n",
    "    # create a dictionary with all the words as keys and 0 as value\n",
    "    reviews = [dict(Counter(review)) for review in reviews]\n",
    "    idfDict = dict.fromkeys(set().union(*(review.keys() for review in reviews)),0)\n",
    "    # TODO: compute inverse document frequency\n",
    "    # Hint: Don't forget to apply logarithm transformation \n",
    "    for document in reviews:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "        \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = np.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "# This function computes the TF-IDF score of all the words in a review\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "#        idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "# Output: tfidf - a dictionary containing the TF-IDF scores of all the words in a review\n",
    "def computeTFIDF(bagOfWords, idfDict):\n",
    "    tfidf = {}  \n",
    "    tfDict = computeTF(bagOfWords)\n",
    "    # TODO: compute tf-idf\n",
    "    for word, val in tfDict.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "  \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReibzsO3f11C",
    "outputId": "1b5d70bf-7134-44de-b3da-ea43b5bfda81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:10<00:00, 4836.63it/s]\n"
     ]
    }
   ],
   "source": [
    "idfs = computeIDF(X)\n",
    "tfidf = X.progress_apply(computeTFIDF, idfDict=idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKq9_e7mephQ"
   },
   "source": [
    "After calculating the TF-IDF scores of all words in all the reviews, we need to map each review to a vector of scores. Notice that the dimension of each vector is the length of total unique words. Run the following cell. This might take 5 mins to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lwsscvq0eFjI"
   },
   "outputs": [],
   "source": [
    "def vectorized(tfidf_doc):\n",
    "    vec = np.zeros(len(total_vocab))\n",
    "    for word, val in tfidf_doc.items():\n",
    "        ind = total_vocab.index(word)\n",
    "        vec[ind] = val\n",
    "    return vec\n",
    "\n",
    "X_vectorized = list(tfidf.progress_apply(vectorized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTN1yvT9zuk2"
   },
   "source": [
    "You have successfully transformed each review to a vector of real numbers. Congrats! \n",
    "\n",
    "Next, we need to apply the vectorized reviews to our sentimental classification problem. We choose one of the easiest models - Naive Bayes classifier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA4u0_361QMP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_train)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5M7LRI59mBz"
   },
   "source": [
    "As you can see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n99SjXCwV1XH"
   },
   "source": [
    "### **Part 3: Co-Occurrence Matrix with a fixed context window**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khfQQkVqWO-t"
   },
   "source": [
    "Words co-occurrence matrix describes how words occur together that in turn captures the relationships between words. Words co-occurrence matrix is computed simply by counting how two or more words occur together in a given corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNu_lUvsVy-p"
   },
   "outputs": [],
   "source": [
    "# TODD \n",
    "# Compute the co-occurance matrix of the corpus X\n",
    "# Input: X - the tokenized sentences \n",
    "#        vocab - the dictionary which maps a word to its index in the words list\n",
    "#        window_size - the size of the context window\n",
    "def co_occ(X, vocab, window_size):\n",
    "    n = len(vocab)\n",
    "    matrix = np.zeros((n,n))\n",
    "    for sentence in X:\n",
    "        for i,word in enumerate(sentence):\n",
    "            for j in range(max(i-window_size,0),min(i+window_size,len(sentence))):\n",
    "                matrix[vocab[word]][sentence[j]]+=1\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZusuqHnkWwpY"
   },
   "source": [
    "Note that Co-Occurrance matrix is not a word embedding that is generally used. Use PCA to decompose the matrix and reduce to K dimension. In addition, to maintain the semantic meaning of the word representation, K is preferred to be in the order of hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "844bpWmGZSQm"
   },
   "outputs": [],
   "source": [
    "co_matrix = co_occ(X, vocab, 5)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "X_pca = pca.transform(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt8TKXqhDcmB"
   },
   "source": [
    "###**Part 4: Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dajg-I2r5khr"
   },
   "source": [
    "Word2Vec is a popular technique of natural language processing to create word embeddings using neural networks, which can be further used for various text related tasks, e.g. sentiment anlaysis. In this problem, we will walk you through the implementation of Skip-Gram model, and the use of Gensim, a popular pacakge for Word2Vec. \n",
    "<br>   \n",
    "In week 2 of 16ML, you have learned the mechanism of gradient descent as well as the implementation of stochastic gradient descent. In this problem, we will let you implement parts of the train function, including generating input and target data from raw tokenized sentences, and compute losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLl8I8y0xj22"
   },
   "source": [
    "**(a)**\n",
    "The training objective (for one training sample) is to maximize the conditional probability of observing the actual context word $w_{Oj}$ given the input context word $w_I$.\n",
    "\\begin{equation*}\n",
    "\\ P(w_{O,1}, w_{O,2}, ... , w_{O,C}| w_I) = \\prod_{c=1}^C\\frac{e^{u_{j^*_c}}}{\\sum_{j'}^V e^{u_{j'}}} \n",
    "\\end{equation*}\n",
    "Where $u$ is the output layer before softmax, and $u_{j^*_c}$ is the node of $c^{th}$ context word in the output layer.\n",
    "<br> Derive the loss function $E$ by taking the negative log of the above expression, and further prove that $$\\frac{\\partial E}{\\partial u_j} = t_j - y_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91Uzph1NxoNK"
   },
   "source": [
    "**(b)** The softmax function is a function that turns a vector of V real values into a vector of the same length that sums to 1, and each value is between 0 and 1, so that they can be interpreted as probabilities. Implement softmax function in the next cell. \n",
    "\\begin{equation*}\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j^V e^{z_j}}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtzMhWbdxsEq"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ## TODD:\n",
    "    #  Compute softmax values for each sets of scores in x.\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7TiIMcm9qDk"
   },
   "source": [
    "**(c)**\n",
    "The following cell is the implementation of Word2Vec (Skip-Gram model) using stochastic gradient descent. Fill in the missing part labeled as TODD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkyqjFMqxxZu"
   },
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self, eta = 0.005, window_size = 2, hidden_num = 10): \n",
    "      # self.N - the number of neurons in the single hidden layer\n",
    "      # self.words - the list of words\n",
    "      # self.eta - the learning rate\n",
    "      # self.vocab - the dictionary which maps the word to its index of self.words\n",
    "      # self.V - the length of the words list\n",
    "      # self.W1 - the first weight matrix of dimension |V| * N\n",
    "      # self.W2 - the second weight matrix of dimension N * |V|\n",
    "      # self.h - the hidden layer\n",
    "      # self.u - the output layer\n",
    "\n",
    "        self.N = hidden_num\n",
    "        self.window_size = window_size\n",
    "        self.eta = eta\n",
    "        self.words = [] \n",
    "        self.vocab = {}\n",
    "        self.V = None\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.h = None\n",
    "        self.u = None\n",
    "        \n",
    "\n",
    "    def initialize(self, X):\n",
    "        # TODD\n",
    "        # build the vocabulary from the tokenized sentences and update self.V, self.words\n",
    "        data = {} \n",
    "        for tokens in X: \n",
    "            for word in tokens: \n",
    "                if word not in data: \n",
    "                    data[word] = 1\n",
    "                else: \n",
    "                    data[word] += 1\n",
    "                    \n",
    "        self.V = len(data) \n",
    "        self.words = sorted(list(data.keys())) \n",
    "        \n",
    "        for i in range(len(data)): \n",
    "            self.vocab[self.words[i]] = i \n",
    "   \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "\n",
    "\n",
    "    # TODD\n",
    "    # Implement the forward function, update self.h and self.u, and return the result of softmax classification\n",
    "    # X - the one hot vectors of center words\n",
    "    def forward(self,X): \n",
    "        self.h = np.dot(self.W1.T,X).reshape(self.N,1) \n",
    "        self.u = np.dot(self.W2.T,self.h) \n",
    "\n",
    "        return softmax(self.u)   \n",
    "  \n",
    "    # TODD\n",
    "    # Implement the backward function, update self.W1 and self.W2\n",
    "    # input: X - the one hot vector of center word\n",
    "    #        y - the output of softmax classification\n",
    "    #        target - the representation of center word's context words in a single vector\n",
    "    def backward(self,X,y,target): \n",
    "        du = y - np.array(target).reshape(-1,1) \n",
    "        dw2 = np.dot(self.h,du.T) \n",
    "        dh = np.dot(self.W2,du)\n",
    "        dw1 = np.dot(np.array(X).reshape(-1,1), dh.T) \n",
    "\n",
    "        self.W2 = self.W2 - self.eta * dw2\n",
    "        self.W1 = self.W1 - self.eta * dw1 \n",
    "           \n",
    "    def train(self,epochs, X):\n",
    "        numTokens = len(sum(X, []))\n",
    "        iterable = range(numTokens)\n",
    "        itbar = tqdm(iterable)\n",
    "        for e in epochs: \n",
    "            itbar.refresh()\n",
    "            itbar.reset()\n",
    "            self.loss = 0\n",
    "            vocab = self.vocab\n",
    "\n",
    "            for sentence in X: \n",
    "                for i in range(len(sentence)): \n",
    "                    itbar.update()\n",
    "                    \n",
    "                    ## TODD\n",
    "                    # You need to process tokens of sentences (self.X) into valid training points\n",
    "                    # 1. the input x (center_word) is an one hot encoding vector of the length of the vocabulary  \n",
    "                    # 2. the output y (context_word) is of the same length of x, which has 1's in the sliding window of\n",
    "                    #    the center_word except the center_word itself. Be catious of the boundary\n",
    "                    \n",
    "                    center_word = [0 for x in range(self.V)] \n",
    "                    center_word[vocab[sentence[i]]] = 1\n",
    "                    context = [0 for x in range(self.V)] \n",
    "\n",
    "                    for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                        if i!=j and j>=0 and j<len(sentence): \n",
    "                            context[vocab[sentence[j]]] += 1\n",
    "                    \n",
    "                    y = self.forward(center_word) \n",
    "                    self.backward(center_word, y, context) \n",
    "                    \n",
    "                    ## TODD\n",
    "                    # compute the loss based on the formula you derived\n",
    "                    C = 0\n",
    "                    for m in range(self.V): \n",
    "                        if(context[m]): \n",
    "                            self.loss += -1*self.u[m][0] \n",
    "                            C += 1\n",
    "                    self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "\n",
    "            print(\"epoch \",e+1, \" loss = \",self.loss) \n",
    "            self.eta *= 1/( (1+self.eta*(e+1)) ) \n",
    "\n",
    "    # TODD\n",
    "    # return context words of the given center word or None if word isn't in the vocabulary\n",
    "    # Input: word - the center word\n",
    "    #        num - the number of context words to return\n",
    "    def predict(self,word,num): \n",
    "        if word in self.words: \n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[self.vocab[word]] = 1\n",
    "            prediction = self.forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=num): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpYEGerQyqNA"
   },
   "source": [
    "**(d)**\n",
    "Because word2vec takes long time to train, you only use the following experiment to test your implementation. Run the next cell and train a word2vec embedding model using your implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuWKDezNyfkt"
   },
   "outputs": [],
   "source": [
    "X_w2v = X[:50]\n",
    "w2v = word2vec() \n",
    "w2v.initialize(X_w2v)\n",
    "\n",
    "# train 100 epochs, you should see your loss is contantly decreasing\n",
    "epochs = tqdm(range(100))\n",
    "w2v.train(epochs,X_w2v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VkKpG2X0Ajb"
   },
   "source": [
    "Check your context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WWnJuxn0A1x"
   },
   "outputs": [],
   "source": [
    "# check your context words\n",
    "print(w2v.predict(\"good\",3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL5pnmxm08An"
   },
   "source": [
    "**(e)**\n",
    "Gensim is a useful library to train your word2vec model faster. Run the following cell to train your Word2Vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2B35y0C1UoE"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(X)\n",
    "\n",
    "# model['word'] will give the embedding of the word \"word\"\n",
    "model['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p6ECJS51bFg"
   },
   "source": [
    "**(f)**\n",
    "After you finish training your word2vec model with gensim, vectorize your tokenized sentences. For each sentence as a data point, we simply average the embedding of each word in this sentence. Implement the vectorize_word2vec function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv1YVDqm1Za-"
   },
   "outputs": [],
   "source": [
    "# Use trained word2vec model to vectorize the sentences. \n",
    "def vectorize_word2vec(X, model):\n",
    "    ## TODD\n",
    "    # Input: X - the tokenized sentences\n",
    "    #        model - the trained word2vec model\n",
    "    # return X_vectorized of dimension N * dim_embedding\n",
    "\n",
    "    X_vectorized = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        X_words = []\n",
    "        for word in X[i]:\n",
    "            if(word in model):\n",
    "                X_words.append(model[word])\n",
    "        X_vectorized.append(np.mean(np.array(X_words), axis = 0).reshape(-1))\n",
    "    return np.array(X_vectorized)\n",
    "\n",
    "X_vectorized = vectorize_word2vec(X, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pofxb9S1YFGV"
   },
   "source": [
    "Check the performance on the task of movie sentiment analysis with word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCXQJ6ykYEZB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9w_vapDDiBm"
   },
   "source": [
    "**Beyond Word2vec: Capturing Word Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDWzXc-QB_oq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EECS289_Final_Project_T.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
