{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1w6S2tf_1r4"
   },
   "source": [
    "# **EECS 16ML Assignment: Traditional/Contextual Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE0fNvxdAfxt"
   },
   "source": [
    "Before you start, make sure you review the lecture slide and notes about word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have any ImportError, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the packages/modules that you may not have\n",
    "# !pip install <PACKAGE_NAME>\n",
    "!pip install nltk\n",
    "!pip install bs4\n",
    "!pip install tqdm\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGllTplLAP5V",
    "outputId": "019b20ca-282a-4748-cdcd-cf2f9a8c10e9"
   },
   "outputs": [],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# if error appears, uncomment the next line\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "# Bert packages\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from keras.layers import Dense,Input\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM4Mwv-cXoEW"
   },
   "outputs": [],
   "source": [
    "# This function clean the input text by removing HTML contents, punctuation, stopwords, then lower-case each word\n",
    "stopWords = set(stopwords.words('english'))\n",
    "def preprocess_text(review):\n",
    "    # remove HTML contents.\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    review = soup.get_text()\n",
    "\n",
    "    # remove everything except lower/upper case letters using Regular Expressions.\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "\n",
    "    # bring everything into lowercase.\n",
    "    review = review.lower()\n",
    "\n",
    "    review = review.split()\n",
    "    review = [word for word in review if not word in stopWords]\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use IMDB dataset of 50,000 labeled movie reviews from [here](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) to perform classification task and compare performance using different word embedding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "vkPT8RaPHKT2",
    "outputId": "f87315fa-dacc-4b4d-d1ee-475c387e5fab"
   },
   "outputs": [],
   "source": [
    "# Reading the dataset and visualize the first ten data points\n",
    "path_data = 'Data/IMDB Dataset.csv'\n",
    "imdb_data = pd.read_csv(path_data)\n",
    "imdb_data.sentiment.replace(\"positive\" , 1 , inplace = True)\n",
    "imdb_data.sentiment.replace(\"negative\" , 0 , inplace = True)\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = imdb_data['review']\n",
    "y = imdb_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXA3S5ocKsPn",
    "outputId": "90cd774c-cca3-4918-daf1-6884ee334f8e"
   },
   "outputs": [],
   "source": [
    "# Clean the raw data\n",
    "tqdm.pandas()\n",
    "X = X_raw.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGIb63I8EWq4"
   },
   "source": [
    "### **Part 1: Word-Count Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHO8G9pI_mL5"
   },
   "source": [
    "As explained in the lecture and notes, Bag of Words, which uses counts of word occurrences to transform texts into vectors of numbers, is an introductory and traditional count-based skill in text representation.\n",
    "\n",
    "**(a)** Define `vectorize` and `vectorizeDF` below to implement the Bag of Words approach, then apply `vectorizeDF` on the first 5,000 reviews.\n",
    "\n",
    "*hint: use `.value_counts()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt_a--1L-ZPZ"
   },
   "outputs": [],
   "source": [
    "# For now, use only first 5000 reviews\n",
    "reviews_5000 = X[:5000]\n",
    "y_5000 = y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF-BW6TOQ9mm"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a vector by counting the occurences of every word in the list\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: vectorOfWords - a Pandas series containing occurences of every word in the input list\n",
    "def vectorize(bagOfWords):\n",
    "    #TODO \n",
    "    return pd.Series(bagOfWords).value_counts()\n",
    "\n",
    "# This function applies vectorize() and vectorize all lists of words included in a Pandas dataframe\n",
    "# Input: reviews - a Pandas dataframe containing all the reviews in *list of words* form\n",
    "# Output: X_vectorized - a Pandas dataframe containing vectorized form of all reviews\n",
    "def vectorizeDF(reviews):\n",
    "    #TODO\n",
    "    X_vectorized = reviews.progress_apply(vectorize)\n",
    "    return X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtRFjFAj486_"
   },
   "outputs": [],
   "source": [
    "X_vectorized = vectorizeDF(reviews_5000).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgIbAsTn5RZa"
   },
   "source": [
    "Check if applying implemented `vectorize` and `vectorizeDF` to out lists of words produce correct length of vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egYy1gT-5TxS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(set(reviews_5000.explode())) == len(X_vectorized.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZcBKzBi5WHl"
   },
   "source": [
    "Then, we use this vectorized data to train one of the most common models - SVM classifier and report accuracy. Your result should be around 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqc-WKKO5Ys8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_5000, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGfd9bw_RFhH"
   },
   "source": [
    "Counting single word occurrences can cost us some useful information in sentences, especially those words that have collective meanings when put together. For example, \"machine learning\" has a special meaning that words \"machine\" and \"learning\" cannot capture. In this case, we want to also count occurrences of each of the two words (bigrams) or multiple words (ngrams) that co-occur in texts.\n",
    "\n",
    "**(b)** Define `bigram()` and `sixgram()` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmspZgtjOw_Z"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a list of 2-grams\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: bagOf2grams - a list of 2-grams: [(word1, word2), (word2, word3) ... ]\n",
    "def bigram(bagOfWords):\n",
    "    #TODO\n",
    "    return [(bagOfWords[i], bagOfWords[i+1]) for i in range(len(bagOfWords)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahQylR3HO6VP"
   },
   "outputs": [],
   "source": [
    "reviews_5000.apply(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy3EFvb2RIuW"
   },
   "source": [
    "`nltk` is a popular package that engineers use in the real world for text data pre-processing [(check documentation here)](http://www.nltk.org/api/nltk.html?highlight=ngram). It also provides a method `ngrams()` that can produce n-grams for us. Try utilize this method and define `sixgram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dmjzOU0RLV6"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2jXICGbRSMh"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a list of 6-grams\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: bagOf6grams - a list of 6-grams: [(word1, word2, word3, word4, word5, word6) ... ]\n",
    "def sixgram(bagOfWords):\n",
    "    #TODO\n",
    "    return list(ngrams(bagOfWords, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKdEpgtJRyro"
   },
   "outputs": [],
   "source": [
    "reviews_5000.apply(sixgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZN2ZVPe5z0S"
   },
   "source": [
    "In the real world, implementing Bag of Words from scratch can be computation-heavy, especially on large dataset with longer texts. Fortunately, `scikit-learn` has implemented it in an efficient way for us with `CountVectorizer` [(check documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "**(c)** Now using `CountVectorizer`, redefine `vectorizeDF()` and apply it on the whole dataset. Play with different parameters like `ngram_range` and `max_features`. **Report the parameters you try and the corresponding accuracy (at least two different combinations).** Note that `CountVectorizer` has a method `get_features_name()` that shows all the word tokens after vectorized if you want to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ip_3b_xm6QaY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Modify our review data into string sentence form\n",
    "reviews = X.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuEibO6C9iV5"
   },
   "outputs": [],
   "source": [
    "# This function vectorize all texts included in a Pandas dataframe\n",
    "# Input: reviews - a list containing all the reviews in *string sentence* form\n",
    "# Output: X_vectorized - a numpy array containing vectorized form of all reviews\n",
    "def vectorizeDF(reviews):\n",
    "    # TODO\n",
    "    # Hint: Remember to add max_features parameter if the dimension of the feature vector is too large\n",
    "    vectorizer = CountVectorizer(ngram_range = (1, 3),max_features=500)\n",
    "    X_vectorized = vectorizer.fit_transform(reviews)\n",
    "    return X_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEymhCHs_XxY"
   },
   "outputs": [],
   "source": [
    "X_vectorized = vectorizeDF(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the training time is quite long for the following cell, it might because the dimension of the feature vector is too large. Try to add `max_features` parameter in `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqc-WKKO5Ys8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start answering part 1(c)\n",
    "Answers may vary among students. Students should give at least two trials.\n",
    "\n",
    "Example:\n",
    "\n",
    "    Parameters: ngram_range = (1, 2), max_features=1000, Accuracy: 85.69%\n",
    "    Parameters: ngram_range = (1, 3), max_features=1000, Accuracy: 86.06%\n",
    "    Parameters: ngram_range = (1, 3), max_features=500, Accuracy: 84.33%\n",
    "\n",
    "#### End answering part 1(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSRpvE80EkPJ"
   },
   "source": [
    "### **Part 2: TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency. It is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Fill in the TODO parts of the following cell to calculate the TF-IDF scores of all words in all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAjiXPMXKKzO"
   },
   "outputs": [],
   "source": [
    "# This function computes the term frequency (TF), i.e. the number of times a word appears in a review divded \n",
    "# by the total number of words in the review.\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: tfDict - a dictionary containing the TF scores of all the words in a review\n",
    "def computeTF(bagOfWords):\n",
    "    # Frequency of each word in a review\n",
    "    wordDict = dict(Counter(bagOfWords))\n",
    "    tfDict = {}\n",
    "    N = len(bagOfWords)\n",
    "    # TODO: compute term frequency\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(N)\n",
    "\n",
    "    return tfDict\n",
    "\n",
    "# This function computes the inverse document frequency (IDF), i.e. the number of reviews divided by the number \n",
    "# of reviews that contain the word w. \n",
    "# Input: reviews - a list containing all the reviews\n",
    "# Output: idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "def computeIDF(reviews):\n",
    "    N = len(reviews)\n",
    "    # create a dictionary with all the words as keys and 0 as value\n",
    "    reviews = [dict(Counter(review)) for review in reviews]\n",
    "    idfDict = dict.fromkeys(set().union(*(review.keys() for review in reviews)),0)\n",
    "    # TODO: compute inverse document frequency\n",
    "    # Hint: Don't forget to apply logarithm transformation \n",
    "    for document in reviews:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = np.log(N / float(val))\n",
    "    \n",
    "    return idfDict\n",
    "\n",
    "# This function computes the TF-IDF score of all the words in a review\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "#        idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "# Output: tfidf - a dictionary containing the TF-IDF scores of all the words in a review\n",
    "def computeTFIDF(bagOfWords, idfDict):\n",
    "    tfidf = {}  \n",
    "    tfDict = computeTF(bagOfWords)\n",
    "    # TODO: compute tf-idf\n",
    "    for word, val in tfDict.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "  \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In consideration of running rime, use only first 5000 reviews\n",
    "reviews_5000 = X[:5000]\n",
    "y_5000 = y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReibzsO3f11C",
    "outputId": "1b5d70bf-7134-44de-b3da-ea43b5bfda81"
   },
   "outputs": [],
   "source": [
    "idfs = computeIDF(reviews_5000)\n",
    "total_vocab = list(idfs.keys())\n",
    "tfidf = reviews_5000.progress_apply(computeTFIDF, idfDict=idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKq9_e7mephQ"
   },
   "source": [
    "After calculating the TF-IDF scores of all words in all the reviews, we need to map each review to a vector of scores. Notice that the dimension of each vector is the length of total unique words. Run the following cell. This might take 10 mins to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lwsscvq0eFjI"
   },
   "outputs": [],
   "source": [
    "def vectorized(tfidf_doc):\n",
    "    vec = np.zeros(len(total_vocab))\n",
    "    for word, val in tfidf_doc.items():\n",
    "        ind = total_vocab.index(word)\n",
    "        vec[ind] = val\n",
    "    return vec\n",
    "\n",
    "X_vectorized = list(tfidf.progress_apply(vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTN1yvT9zuk2"
   },
   "source": [
    "You have successfully transformed each review to a vector of real numbers. Congrats! \n",
    "\n",
    "Next, we need to apply the vectorized reviews to our sentimental classification problem. We choose the same model as last part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA4u0_361QMP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_5000, test_size = 0.2)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, vectorizing the first 5000 reviews using tf-idf methods take quite a long time. In real world, the size of dataset is much larger. Similiar to the last part, `scikit-learn` has implemented it in an efficient way for us with `TfidfVectorizer` [(check documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "**(b)** Apply `TfidfVectorizer` on the whole dataset and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = X.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TODO \n",
    "tv=TfidfVectorizer()\n",
    "tv_vectorized=tv.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tv_vectorized, y, test_size = 0.2,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Comment on the result of TF-IDF with SVM classifier and compare the result with last part. Is the performace better or worse? Explain the reason to the best of your knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting answering part 2(c)\n",
    "The accuracy of TF-IDF with SVM on the entire dataset is 89%.\n",
    "\n",
    "The performance is better than Bag of words because Bag of Words just creates a set of vectors containing the count of word occurrences in the documents (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "\n",
    "#### End answering part 2(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n99SjXCwV1XH"
   },
   "source": [
    "### Part 3: Co-Occurrence Matrix and GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khfQQkVqWO-t"
   },
   "source": [
    "Words co-occurrence matrix describes how words occur together that in turn captures the relationships between words. Words co-occurrence matrix is computed simply by counting how two or more words occur together in a given corpus. \n",
    "\n",
    "**(a)** Fill in the following TODO part to finish implementation of `co_occ()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNu_lUvsVy-p"
   },
   "outputs": [],
   "source": [
    "# Compute the co-occurrence matrix of the corpus X\n",
    "# Input: X - the tokenized sentences \n",
    "#        vocab - the dictionary which maps a word to its index in the words list\n",
    "#        window_size - the size of the context window\n",
    "def co_occ(X, vocab, window_size):\n",
    "    n = len(vocab)\n",
    "    matrix = np.zeros((n,n))\n",
    "    # TODO\n",
    "    for sentence in X:\n",
    "        for i,word in enumerate(sentence):\n",
    "            for j in range(max(i-window_size,0),min(i+window_size,len(sentence))):\n",
    "                matrix[vocab[word]][vocab[sentence[j]]]+=1\n",
    "    # end TODO\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to build the co-occurrence matrix for 500 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for mapping word to its index in the words list\n",
    "X_500 = X[:500]\n",
    "data = {} \n",
    "vocab = {}\n",
    "for tokens in X_500: \n",
    "    for word in tokens: \n",
    "        if word not in data: \n",
    "            data[word] = 1\n",
    "        else: \n",
    "            data[word] += 1\n",
    "\n",
    "words = sorted(list(data.keys()))\n",
    "for i in range(len(data)): \n",
    "    vocab[words[i]] = i\n",
    "\n",
    "\n",
    "# create co-occurrence matrix\n",
    "co_matrix = co_occ(X_500, vocab, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZusuqHnkWwpY"
   },
   "source": [
    "Note that Co-Occurrance matrix is not a word embedding that is generally used. Use PCA to decompose the matrix and reduce to K dimension. In addition, to maintain the semantic meaning of the word representation, K is preferred to be in the order of hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "844bpWmGZSQm"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(co_matrix)\n",
    "co_pca = pca.transform(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformation by PCA, the matrix becomes |V| * K, with each row representing a word embedding. Now you are ready to apply the matrix into the task of movie sentiment analysis. \n",
    "\n",
    "**(b)** Implement `vectorize_co(X, model)` and run the following cells and report resulting accuracy. Here, we will simply average word vectors for sentence representation. (Due to the long fitting time and memory requirement for PCA, we limit the datasets of size 500, the accuracy can improve when the dataset is larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization of tokenized sentences with transformed co-occurrence matrix\n",
    "# Input: X - the tokenized sentences\n",
    "#        model - co-occurrence matrix\n",
    "def vectorize_co(X, model):\n",
    "    X_vectorized = []\n",
    "    # TODO: vectorize each piece of text in X using co-occurrence matrix built\n",
    "    for i in range(len(X)):\n",
    "        X_words = []\n",
    "        for word in X[i]:\n",
    "            if(word in vocab):\n",
    "                X_words.append(model[vocab[word]])\n",
    "        X_vectorized.append(np.mean(np.array(X_words), axis = 0).reshape(-1))\n",
    "    # end TODO\n",
    "    return np.array(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vectorize_co(X, co_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have observed, building co-occurrence matrix and performing PCA ourselves can be time consuming especially when our training set has a large size. As a result, engineers in the industry utilize [GloVe](https://nlp.stanford.edu/projects/glove/). The website provides several different sets of pre-trained word vectors either with different dimensions or using different type of corpus. In other words, someone has built the co-occurrence matrices and performed PCA based on either wiki comments or tweets for us, thus saving us some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Implement `vectorize_glove(X, dic)`. Load both 50-dim and 300-dim pretrained GloVe word vectors, then vectorize review data, perform classification, and report accuracies on both set of GloVe vectors. Again, we will use the average of word vectors for sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the 50-dimension GloVe word vectors \n",
    "embeddings_dict = {}\n",
    "with open(\"Data/glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        embeddings_dict[values[0]] = np.asarray(values[1:], \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vector for the word \"king\"\n",
    "embeddings_dict['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization of tokenized sentences with pretrained GloVe vectors\n",
    "# Input: X - the tokenized sentences\n",
    "#        dic - dictionary of loaded GloVe vectors\n",
    "def vectorize_glove(X, dic):\n",
    "    X_vectorized = []\n",
    "    # TODO\n",
    "    for i in range(len(X)):\n",
    "        X_words = []\n",
    "        for word in X[i]:\n",
    "            if(word in dic.keys()):\n",
    "                X_words.append(dic[word])\n",
    "        X_vectorized.append(np.mean(np.array(X_words), axis = 0).reshape(-1))\n",
    "    # end TODO\n",
    "    return np.array(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vectorize_glove(X, embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt8TKXqhDcmB"
   },
   "source": [
    "### Part 4: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dajg-I2r5khr"
   },
   "source": [
    "Word2Vec is a popular technique of natural language processing to create word embeddings using neural networks, which can be further used for various text related tasks, e.g. sentiment anlaysis. In this problem, we will walk you through the implementation of Skip-Gram model, and the use of Gensim, a popular pacakge for Word2Vec. \n",
    "<br>   \n",
    "In week 2 of 16ML, you have learned the mechanism of gradient descent as well as the implementation of stochastic gradient descent. In this problem, we will let you implement parts of the train function, including generating input and target data from raw tokenized sentences, and compute losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLl8I8y0xj22"
   },
   "source": [
    "**(a)**\n",
    "The training objective (for one training sample) is to maximize the conditional probability of observing the actual context word $w_{Oj}$ given the input context word $w_I$.\n",
    "\\begin{equation*}\n",
    "\\ P(w_{O,1}, w_{O,2}, ... , w_{O,C}| w_I) = \\prod_{c=1}^C\\frac{e^{u_{j^*_c}}}{\\sum_{j'}^V e^{u_{j'}}} \n",
    "\\end{equation*}\n",
    "Where $u$ is the output layer before softmax, and $u_{j^*_c}$ is the node of $c^{th}$ context word in the output layer.\n",
    "<br> Derive the loss function $E$ by taking the negative log of the above expression, and further have that $$\\frac{\\partial E}{\\partial u_j} = t_j - y_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "E &= -\\log p(W_{O,1}, ..., W_{O,C} | W_I) \\\\\n",
    "  &= -\\log \\prod_{c=1}^C\\frac{e^{u_{j^*_c}}}{\\sum_{j'}^{V} e^{u_{j'}}} \\\\\n",
    "  &= -\\sum_{c=1}^C u_{j^*_c} + C * \\log \\sum_{j'=1}^{V} e^{u_{j'}}\n",
    " \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial u_j} = t_j - y_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91Uzph1NxoNK"
   },
   "source": [
    "**(b)** The softmax function is a function that turns a vector of V real values into a vector of the same length that sums to 1, and each value is between 0 and 1, so that they can be interpreted as probabilities. Implement softmax function in the next cell. \n",
    "\\begin{equation*}\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j^V e^{z_j}}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtzMhWbdxsEq"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ## TODO: Compute softmax values for each sets of scores in x.\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7TiIMcm9qDk"
   },
   "source": [
    "**(c)**\n",
    "The following cell is the implementation of Word2Vec (Skip-Gram model) using stochastic gradient descent. Fill in the missing part labeled as TODD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkyqjFMqxxZu"
   },
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self, eta = 0.005, window_size = 2, hidden_num = 10): \n",
    "      # self.N - the number of neurons in the single hidden layer\n",
    "      # self.words - the list of words\n",
    "      # self.eta - the learning rate\n",
    "      # self.vocab - the dictionary which maps the word to its index of self.words\n",
    "      # self.V - the length of the words list\n",
    "      # self.W1 - the first weight matrix of dimension |V| * N\n",
    "      # self.W2 - the second weight matrix of dimension N * |V|\n",
    "      # self.h - the hidden layer\n",
    "      # self.u - the output layer\n",
    "\n",
    "        self.N = hidden_num\n",
    "        self.window_size = window_size\n",
    "        self.eta = eta\n",
    "        self.words = [] \n",
    "        self.vocab = {}\n",
    "        self.V = None\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.h = None\n",
    "        self.u = None\n",
    "        \n",
    "\n",
    "    def initialize(self, X):\n",
    "        # build the vocabulary from the tokenized sentences and update self.V, self.words\n",
    "        data = {} \n",
    "        for tokens in X: \n",
    "            for word in tokens: \n",
    "                if word not in data: \n",
    "                    data[word] = 1\n",
    "                else: \n",
    "                    data[word] += 1\n",
    "                    \n",
    "        self.V = len(data) \n",
    "        self.words = sorted(list(data.keys())) \n",
    "        \n",
    "        for i in range(len(data)): \n",
    "            self.vocab[self.words[i]] = i \n",
    "   \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "\n",
    "\n",
    "    # Implement the forward function, update self.h and self.u, and return the result of softmax classification\n",
    "    # X - the one hot vectors of center words\n",
    "    def forward(self,X): \n",
    "        # TODO: update self.h and self.u, and return the result of softmax classification\n",
    "        self.h = np.dot(self.W1.T,X).reshape(self.N,1) \n",
    "        self.u = np.dot(self.W2.T,self.h) \n",
    "\n",
    "        return softmax(self.u)   \n",
    "  \n",
    "    # Implement the backward function, update self.W1 and self.W2\n",
    "    # input: X - the one hot vector of center word\n",
    "    #        y - the output of softmax classification\n",
    "    #        target - the representation of center word's context words in a single vector\n",
    "    def backward(self,X,y,target): \n",
    "        # TODO: update self.W1 and self.W2\n",
    "        du = y - np.array(target).reshape(-1,1) \n",
    "        dw2 = np.dot(self.h,du.T) \n",
    "        dh = np.dot(self.W2,du)\n",
    "        dw1 = np.dot(np.array(X).reshape(-1,1), dh.T) \n",
    "\n",
    "        self.W2 = self.W2 - self.eta * dw2\n",
    "        self.W1 = self.W1 - self.eta * dw1 \n",
    "           \n",
    "    def train(self,epochs, X):\n",
    "        numTokens = len(sum(X, []))\n",
    "        iterable = range(numTokens)\n",
    "        itbar = tqdm(iterable)\n",
    "        for e in epochs: \n",
    "            itbar.refresh()\n",
    "            itbar.reset()\n",
    "            self.loss = 0\n",
    "            vocab = self.vocab\n",
    "\n",
    "            for sentence in X: \n",
    "                for i in range(len(sentence)): \n",
    "                    itbar.update()\n",
    "                    \n",
    "                    ## TODO\n",
    "                    # You need to process tokens of sentences (self.X) into valid training points\n",
    "                    # 1. the input x (center_word) is an one hot encoding vector of the length of the vocabulary  \n",
    "                    # 2. the output y (context_word) is of the same length of x, which has 1's in the sliding window of\n",
    "                    #    the center_word except the center_word itself. Be catious of the boundary\n",
    "                    \n",
    "                    center_word = [0 for x in range(self.V)] \n",
    "                    center_word[vocab[sentence[i]]] = 1\n",
    "                    context = [0 for x in range(self.V)] \n",
    "\n",
    "                    for j in range(i-w2v.window_size,i+w2v.window_size): \n",
    "                        if i!=j and j>=0 and j<len(sentence): \n",
    "                            context[vocab[sentence[j]]] += 1\n",
    "                    \n",
    "                    y = self.forward(center_word) \n",
    "                    self.backward(center_word, y, context) \n",
    "                    \n",
    "                    ## TODO\n",
    "                    # compute the loss based on the formula you derived\n",
    "                    C = 0\n",
    "                    for m in range(self.V): \n",
    "                        if(context[m]): \n",
    "                            self.loss += -1*self.u[m][0] \n",
    "                            C += 1\n",
    "                    self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
    "\n",
    "            print(\"epoch \",e+1, \" loss = \",self.loss) \n",
    "            self.eta *= 1/( (1+self.eta*(e+1)) ) \n",
    "\n",
    "            \n",
    "    # return context words of the given center word or None if word isn't in the vocabulary\n",
    "    # Input: word - the center word\n",
    "    #        num - the number of context words to return\n",
    "    def predict(self,word,num): \n",
    "        if word in self.words: \n",
    "            # TODD:\n",
    "            X = [0 for i in range(self.V)] \n",
    "            X[self.vocab[word]] = 1\n",
    "            prediction = self.forward(X) \n",
    "            output = {} \n",
    "            for i in range(self.V): \n",
    "                output[prediction[i][0]] = i \n",
    "               \n",
    "            top_context_words = [] \n",
    "            for k in sorted(output,reverse=True): \n",
    "                top_context_words.append(self.words[output[k]]) \n",
    "                if(len(top_context_words)>=num): \n",
    "                    break\n",
    "       \n",
    "            return top_context_words \n",
    "        else: \n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpYEGerQyqNA"
   },
   "source": [
    "**(d)**\n",
    "Because word2vec takes long time to train, you only use the following experiment to test your implementation. Run the next cell and train a word2vec embedding model using your implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuWKDezNyfkt"
   },
   "outputs": [],
   "source": [
    "X_w2v = X[:50]\n",
    "w2v = word2vec() \n",
    "w2v.initialize(X_w2v)\n",
    "\n",
    "# train 100 epochs, you should see your loss is contantly decreasing\n",
    "epochs = tqdm(range(100))\n",
    "w2v.train(epochs,X_w2v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VkKpG2X0Ajb"
   },
   "source": [
    "Check your context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WWnJuxn0A1x"
   },
   "outputs": [],
   "source": [
    "# check your context words\n",
    "print(w2v.predict(\"good\",3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL5pnmxm08An"
   },
   "source": [
    "**(e)**\n",
    "`Gensim` is a useful library to train your word2vec model faster [(check documentation here)](https://radimrehurek.com/gensim/models/word2vec.html). Run the following cell to train your Word2Vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2B35y0C1UoE"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(X)\n",
    "\n",
    "# model['word'] will give the embedding of the word \"word\"\n",
    "model['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p6ECJS51bFg"
   },
   "source": [
    "**(f)**\n",
    "After you finish training your word2vec model with gensim, vectorize your tokenized sentences. For each sentence as a data point, we simply average the embedding of each word in this sentence. Implement the vectorize_word2vec function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv1YVDqm1Za-"
   },
   "outputs": [],
   "source": [
    "# Use trained word2vec model to vectorize the sentences. \n",
    "# Input: X - the tokenized sentences\n",
    "#        model - the trained word2vec model\n",
    "# return X_vectorized of dimension N * dim_embedding\n",
    "def vectorize_word2vec(X, model):\n",
    "    X_vectorized = []\n",
    "    # TODD:\n",
    "    for i in range(len(X)):\n",
    "        X_words = []\n",
    "        for word in X[i]:\n",
    "            if(word in model):\n",
    "                X_words.append(model[word])\n",
    "        X_vectorized.append(np.mean(np.array(X_words), axis = 0).reshape(-1))\n",
    "    return np.array(X_vectorized)\n",
    "\n",
    "X_vectorized = vectorize_word2vec(X, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pofxb9S1YFGV"
   },
   "source": [
    "Check the performance on the task of movie sentiment analysis with word2vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCXQJ6ykYEZB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9w_vapDDiBm"
   },
   "source": [
    "### **Part 5: BERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDWzXc-QB_oq"
   },
   "source": [
    "In the last section, we will introduce a state-of-the-art word embedding model called BERT (Bidirectional Encoder Representations from Transformers). It was created and published in 2018 by Jacob Devlin and his colleagues from Google. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks. If you are interested in the details of the Bert model, you can refer to their paper: https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use pre-trained BertWordPieceTokenizer to preprocess the reviews and the pre_trained TFDistilBertModel to classify the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained tokenizer\n",
    "tokenizer = BertWordPieceTokenizer('Data/vocab.txt', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding()\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "        \n",
    "    # Padding the last chunk\n",
    "    res = np.zeros((len(all_ids), maxlen))\n",
    "    for x in range(len(all_ids)):\n",
    "        for y in range(len(all_ids[x])):\n",
    "            res[x][y] = all_ids[x][y]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pre_trained tokenizer on our dataset\n",
    "X_raw_5000 = X_raw[:5000]\n",
    "y_5000 = y[:5000]\n",
    "X_tokenized = fast_encode(X_raw_5000.values, tokenizer, maxlen=400)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tokenized, y_5000, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=400):  \n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(name='Movie_Reviews_Classification',inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre_trained tokenizer\n",
    "bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# Customize the pre_trained model to fullfill our task\n",
    "model = build_model(bert_model, max_len=400)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After intializing the model, we need to apply it on the movie reviews dataset. Since the training process takes hours to run with CPU, we have trained the result for you. You can skip the next cell and load the results in the following cell directly. If you want to experiment with the training process, feel free to uncomment the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line takes hours to run, so don't uncomment it unless you want to experiment the training process.\n",
    "# model.fit(X_train,y_train,batch_size = 32 ,validation_data=(X_test,y_test),epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"Data/bert_model\")\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.load('Data/my_history.npy',allow_pickle='TRUE').item()\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "\n",
    "axes[0].plot(history['accuracy'])\n",
    "axes[0].plot(history['val_accuracy'])\n",
    "axes[0].set_title('model accuracy')\n",
    "axes[0].set_ylabel('accuracy')\n",
    "axes[0].set_xlabel('epoch')\n",
    "\n",
    "\n",
    "axes[1].plot(history['loss'])\n",
    "axes[1].plot(history['val_loss'])\n",
    "axes[1].set_title('model loss')\n",
    "axes[1].set_ylabel('loss')\n",
    "axes[1].set_xlabel('epoch')\n",
    "\n",
    "fig.legend(['train', 'val'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Comment on the result of Bert model and compare it with the previous models (Remember we only train the first 5000 reviews in Bert model).\n",
    "\n",
    "(b) Comment on the graph of model accuracy and loss. What do you think will happen if we run the model with more ephochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start answering part5\n",
    "(a) The accuracy of Bert model is 89.7%. Though it only has 5000 reviews, the performance is almost the same or even better than all the other methods with entire dataset. \n",
    "\n",
    "(b) As the number of epoches increases, the accuracy increases and the validation loss first decreases and then increases. From the graph, we notice that the marginal increase of accuracy is decreasing. Therefore, if we have more ephoches, the accuracy might still increase, but the margin increase of accuracy might decrease. Also, since the validation loss increases from ephoch 1 to 2, running more ephoches might not help the performance much.\n",
    "#### End answering part5"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EECS289_Final_Project_T.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
