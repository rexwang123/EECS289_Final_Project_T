{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1w6S2tf_1r4"
   },
   "source": [
    "# **EECS 16ML Assignment: Traditional/Contextual Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE0fNvxdAfxt"
   },
   "source": [
    "Before you start, make sure you review the lecture slide and notes about word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGllTplLAP5V",
    "outputId": "019b20ca-282a-4748-cdcd-cf2f9a8c10e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# if error appears, uncomment the next line\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "# Bert packages\n",
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from keras.layers import Dense,Input\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OM4Mwv-cXoEW"
   },
   "outputs": [],
   "source": [
    "# This function clean the input text by removing HTML contents, punctuation, stopwords, then lower-case each word\n",
    "stopWords = set(stopwords.words('english'))\n",
    "def preprocess_text(review):\n",
    "    # remove HTML contents.\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    review = soup.get_text()\n",
    "\n",
    "    # remove everything except lower/upper case letters using Regular Expressions.\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "\n",
    "    # bring everything into lowercase.\n",
    "    review = review.lower()\n",
    "\n",
    "    review = review.split()\n",
    "    review = [word for word in review if not word in stopWords]\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "vkPT8RaPHKT2",
    "outputId": "f87315fa-dacc-4b4d-d1ee-475c387e5fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "5  Probably my all-time favorite movie, a story o...          1\n",
       "6  I sure would like to see a resurrection of a u...          1\n",
       "7  This show was an amazing, fresh & innovative i...          0\n",
       "8  Encouraged by the positive comments about this...          0\n",
       "9  If you like original gut wrenching laughter yo...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset and visualize the first ten data points\n",
    "path_data = 'IMDB Dataset.csv'\n",
    "imdb_data = pd.read_csv(path_data)\n",
    "imdb_data.sentiment.replace(\"positive\" , 1 , inplace = True)\n",
    "imdb_data.sentiment.replace(\"negative\" , 0 , inplace = True)\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = imdb_data['review']\n",
    "y = imdb_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXA3S5ocKsPn",
    "outputId": "90cd774c-cca3-4918-daf1-6884ee334f8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ab4d9a67c14d5bb733c0de331da640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the raw data\n",
    "tqdm.pandas()\n",
    "X = X_raw.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGIb63I8EWq4"
   },
   "source": [
    "### **Part 1: Word-Count Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHO8G9pI_mL5"
   },
   "source": [
    "As explained in the lecture and notes, Bag of Words, which uses counts of word occurrences to transform texts into vectors of numbers, is an introductory and traditional count-based skill in text representation.\n",
    "\n",
    "**(a)** Define `vectorize` and `vectorizeDF` below to implement the Bag of Words approach, then apply `vectorizeDF` on the first 5,000 reviews.\n",
    "\n",
    "*hint: use `.value_counts()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vt_a--1L-ZPZ"
   },
   "outputs": [],
   "source": [
    "# For now, use only first 5000 reviews\n",
    "reviews_5000 = X[:5000]\n",
    "y_5000 = y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aF-BW6TOQ9mm"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a vector by counting the occurences of every word in the list\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: vectorOfWords - a Pandas series containing occurences of every word in the input list\n",
    "def vectorize(bagOfWords):\n",
    "    #TODO \n",
    "    return\n",
    "\n",
    "# This function applies vectorize() and vectorize all lists of words included in a Pandas dataframe\n",
    "# Input: reviews - a Pandas dataframe containing all the reviews in *list of words* form\n",
    "# Output: X_vectorized - a Pandas dataframe containing vectorized form of all reviews\n",
    "def vectorizeDF(reviews):\n",
    "    #TODO\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YtRFjFAj486_"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c43238448d64516b40acebb579458c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_vectorized = vectorizeDF(reviews_5000).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgIbAsTn5RZa"
   },
   "source": [
    "Check if applying implemented `vectorize` and `vectorizeDF` to out lists of words produce correct length of vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "egYy1gT-5TxS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(reviews_5000.explode())) == len(X_vectorized.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZcBKzBi5WHl"
   },
   "source": [
    "Then, we use this vectorized data to train one of the most common models - SVM classifier and report accuracy. Your result should be around 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oqc-WKKO5Ys8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy:  0.84\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_5000, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGfd9bw_RFhH"
   },
   "source": [
    "Counting single word occurrences can cost us some useful information in sentences, especially those words that have collective meanings when put together. For example, \"machine learning\" has a special meaning that words \"machine\" and \"learning\" cannot capture. In this case, we want to also count occurrences of each of the two words (bigrams) or multiple words (ngrams) that co-occur in texts.\n",
    "\n",
    "**(b)** Define `bigram()` and `sixgram()` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QmspZgtjOw_Z"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a list of 2-grams\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: bagOf2grams - a list of 2-grams: [(word1, word2), (word2, word3) ... ]\n",
    "def bigram(bagOfWords):\n",
    "    #TODO\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ahQylR3HO6VP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(one, reviewers), (reviewers, mentioned), (me...\n",
       "1       [(wonderful, little), (little, production), (p...\n",
       "2       [(thought, wonderful), (wonderful, way), (way,...\n",
       "3       [(basically, family), (family, little), (littl...\n",
       "4       [(petter, mattei), (mattei, love), (love, time...\n",
       "                              ...                        \n",
       "4995    [(interesting, slasher), (slasher, film), (fil...\n",
       "4996    [(watched, series), (series, first), (first, c...\n",
       "4997    [(jet, li), (li, brings), (brings, charismatic...\n",
       "4998    [(rented, movie), (movie, hearing), (hearing, ...\n",
       "4999    [(big, disappointment), (disappointment, think...\n",
       "Name: review, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_5000.apply(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy3EFvb2RIuW"
   },
   "source": [
    "`nltk` is a popular package that engineers use in the real world for text data pre-processing [(check documentation here)](http://www.nltk.org/api/nltk.html?highlight=ngram). It also provides a method `ngrams()` that can produce n-grams for us. Try utilize this method and define `sixgram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1dmjzOU0RLV6"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "u2jXICGbRSMh"
   },
   "outputs": [],
   "source": [
    "# This function turns a list of words into a list of 6-grams\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: bagOf6grams - a list of 6-grams: [(word1, word2, word3, word4, word5, word6) ... ]\n",
    "def sixgram(bagOfWords):\n",
    "    #TODO\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sKdEpgtJRyro"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(one, reviewers, mentioned, watching, oz, epi...\n",
       "1       [(wonderful, little, production, filming, tech...\n",
       "2       [(thought, wonderful, way, spend, time, hot), ...\n",
       "3       [(basically, family, little, boy, jake, thinks...\n",
       "4       [(petter, mattei, love, time, money, visually)...\n",
       "                              ...                        \n",
       "4995    [(interesting, slasher, film, multiple, suspec...\n",
       "4996    [(watched, series, first, came, years, old), (...\n",
       "4997    [(jet, li, brings, charismatic, presence, movi...\n",
       "4998    [(rented, movie, hearing, chris, gore, saying)...\n",
       "4999    [(big, disappointment, think, worst, mastroian...\n",
       "Name: review, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_5000.apply(sixgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZN2ZVPe5z0S"
   },
   "source": [
    "In the real world, implementing Bag of Words from scratch can be computation-heavy, especially on large dataset with longer texts. Fortunately, `scikit-learn` has implemented it in an efficient way for us with `CountVectorizer` [(check documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "**(c)** Now using `CountVectorizer`, redefine `vectorizeDF()` and apply it on the whole dataset. Play with different parameters like `ngram_range` and `max_features`. Report the parameters you try and the corresponding accuracy. (Note that `CountVectorizer` has a method `get_features_name()` that shows all the word tokens after vectorized if you want to take a look)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ip_3b_xm6QaY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Modify our review data into string sentence form\n",
    "reviews = X.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AuEibO6C9iV5"
   },
   "outputs": [],
   "source": [
    "# This function vectorize all texts included in a Pandas dataframe\n",
    "# Input: reviews - a list containing all the reviews in *string sentence* form\n",
    "# Output: X_vectorized - a numpy array containing vectorized form of all reviews\n",
    "def vectorizeDF(reviews):\n",
    "    # TODO\n",
    "    # Hint: Remember to add max_features parameter if the dimension of the feature vector is too large\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OEymhCHs_XxY"
   },
   "outputs": [],
   "source": [
    "X_vectorized = vectorizeDF(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the training time is quite long for the following cell, it might because the dimension of the feature vector is too large. Try to add `max_features` parameter in `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oqc-WKKO5Ys8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy:  0.8607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start answering part 1(c)\n",
    "\n",
    "#### End answering part 1(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSRpvE80EkPJ"
   },
   "source": [
    "### **Part 2: TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency. It is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Fill in the TODO parts of the following cell to calculate the TF-IDF scores of all words in all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "xAjiXPMXKKzO"
   },
   "outputs": [],
   "source": [
    "# This function computes the term frequency (TF), i.e. the number of times a word appears in a review divded \n",
    "# by the total number of words in the review.\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: tfDict - a dictionary containing the TF scores of all the words in a review\n",
    "def computeTF(bagOfWords):\n",
    "    # Frequency of each word in a review\n",
    "    wordDict = dict(Counter(bagOfWords))\n",
    "    tfDict = {}\n",
    "    N = len(bagOfWords)\n",
    "    # TODO: compute term frequency\n",
    "    # Hint: Don't forget to apply logarithm transformation \n",
    "\n",
    "    return tfDict\n",
    "\n",
    "# This function computes the inverse document frequency (IDF), i.e. the number of reviews divided by the number \n",
    "# of reviews that contain the word w. \n",
    "# Input: reviews - a list containing all the reviews\n",
    "# Output: idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "def computeIDF(reviews):\n",
    "    N = len(reviews)\n",
    "    # create a dictionary with all the words as keys and 0 as value\n",
    "    reviews = [dict(Counter(review)) for review in reviews]\n",
    "    idfDict = dict.fromkeys(set().union(*(review.keys() for review in reviews)),0)\n",
    "    # TODO: compute inverse document frequency\n",
    "    # Hint: Don't forget to apply logarithm transformation \n",
    "    \n",
    "    return idfDict\n",
    "\n",
    "# This function computes the TF-IDF score of all the words in a review\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "#        idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "# Output: tfidf - a dictionary containing the TF-IDF scores of all the words in a review\n",
    "def computeTFIDF(bagOfWords, idfDict):\n",
    "    tfidf = {}  \n",
    "    tfDict = computeTF(bagOfWords)\n",
    "    # TODO: compute tf-idf\n",
    "  \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In consideration of running rime, use only first 5000 reviews\n",
    "reviews_5000 = X[:5000]\n",
    "y_5000 = y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReibzsO3f11C",
    "outputId": "1b5d70bf-7134-44de-b3da-ea43b5bfda81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0f421ae9f6477795a576d819d61d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "idfs = computeIDF(reviews_5000)\n",
    "total_vocab = list(idfs.keys())\n",
    "tfidf = reviews_5000.progress_apply(computeTFIDF, idfDict=idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKq9_e7mephQ"
   },
   "source": [
    "After calculating the TF-IDF scores of all words in all the reviews, we need to map each review to a vector of scores. Notice that the dimension of each vector is the length of total unique words. Run the following cell. This might take 10 mins to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Lwsscvq0eFjI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ea14a7f9b64159b71b72dbafb52be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def vectorized(tfidf_doc):\n",
    "    vec = np.zeros(len(total_vocab))\n",
    "    for word, val in tfidf_doc.items():\n",
    "        ind = total_vocab.index(word)\n",
    "        vec[ind] = val\n",
    "    return vec\n",
    "\n",
    "X_vectorized = list(tfidf.progress_apply(vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTN1yvT9zuk2"
   },
   "source": [
    "You have successfully transformed each review to a vector of real numbers. Congrats! \n",
    "\n",
    "Next, we need to apply the vectorized reviews to our sentimental classification problem. We choose the same model as last part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "cA4u0_361QMP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy:  0.869\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_5000, test_size = 0.2)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, vectorizing the first 5000 reviews using tf-idf methods take quite a long time. In real world, the size of dataset is much larger. Similiar to the last part, `scikit-learn` has implemented it in an efficient way for us with `TfidfVectorizer` [(check documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "**(b)** Apply `TfidfVectorizer` on the whole dataset and report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = X.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TODO \n",
    "# tv_vectorized="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tv_vectorized, y, test_size = 0.2,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Comment on the result of TF-IDF with SVM classifier and compare the result with last part. Is the performace better or worse? Explain the reason to the best of your knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting answering part 2(c)\n",
    "\n",
    "#### End answering part 2(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, vectorizing the first 5000 reviews using tf-idf methods take quite a long time. In real world, the size of dataset is much large. Similiar to the last part, `scikit-learn` has implemented it in an efficient way for us with `TfidfVectorizer` [(check documentation here)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Feel free to try `TfidfVectorizer` with the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n99SjXCwV1XH"
   },
   "source": [
    "### Part 3: Co-Occurrence Matrix and GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khfQQkVqWO-t"
   },
   "source": [
    "Words co-occurrence matrix describes how words occur together that in turn captures the relationships between words. Words co-occurrence matrix is computed simply by counting how two or more words occur together in a given corpus. \n",
    "\n",
    "**(a)** Fill in the following TODO part to finish implementation of `co_occ()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNu_lUvsVy-p"
   },
   "outputs": [],
   "source": [
    "# Compute the co-occurrence matrix of the corpus X\n",
    "# Input: X - the tokenized sentences \n",
    "#        vocab - the dictionary which maps a word to its index in the words list\n",
    "#        window_size - the size of the context window\n",
    "def co_occ(X, vocab, window_size):\n",
    "    n = len(vocab)\n",
    "    matrix = np.zeros((n,n))\n",
    "    # TODO\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to build the co-occurrence matrix for 500 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for mapping word to its index in the words list\n",
    "X_500 = X[:500]\n",
    "data = {} \n",
    "vocab = {}\n",
    "for tokens in X_500: \n",
    "    for word in tokens: \n",
    "        if word not in data: \n",
    "            data[word] = 1\n",
    "        else: \n",
    "            data[word] += 1\n",
    "\n",
    "words = sorted(list(data.keys()))\n",
    "for i in range(len(data)): \n",
    "    vocab[words[i]] = i\n",
    "\n",
    "\n",
    "# create co-occurrence matrix\n",
    "co_matrix = co_occ(X_500, vocab, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZusuqHnkWwpY"
   },
   "source": [
    "Note that Co-Occurrance matrix is not a word embedding that is generally used. Use PCA to decompose the matrix and reduce to K dimension. In addition, to maintain the semantic meaning of the word representation, K is preferred to be in the order of hundreds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "844bpWmGZSQm"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "co_pca = pca.transform(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformation by PCA, the matrix becomes |V| * K, with each row representing a word embedding. Now you are ready to apply the matrix into the task of movie sentiment analysis. \n",
    "\n",
    "**(b)** Implement `vectorize_co(X, model)` and run the following cells and report resulting accuracy. (Due to the long fitting time and memory requirement for PCA, we limit the datasets of size 500, the accuracy can improve when the dataset is larger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization of tokenized sentences with transformed co-occurrence matrix\n",
    "# Input: X - the tokenized sentences\n",
    "#        model - co-occurrence matrix\n",
    "def vectorize_co(X, model):\n",
    "    X_vectorized = []\n",
    "    # TODO: vectorize each piece of text in X using co-occurrence matrix built\n",
    "\n",
    "    return np.array(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vectorize_co(X, co_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have observed, building co-occurrence matrix and performing PCA ourselves can be time consuming especially when our training set has a large size. As a result, engineers in the industry utilize [GloVe](https://nlp.stanford.edu/projects/glove/). The website provides several different sets of pre-trained word vectors either with different dimensions or using different type of corpus. In other words, someone has built the co-occurrence matrices and performed PCA based on either wiki comments or tweets for us, thus saving us some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Implement `vectorize_glove(X, dic)`. Load both 50-dim and 300-dim pretrained GloVe word vectors, then vectorize review data, perform classification, and report accuracies on both set of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the 50-dimension GloVe word vectors \n",
    "embeddings_dict = {}\n",
    "with open(\"Data/glove.6B.50d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\n",
       "       -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,\n",
       "       -0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\n",
       "       -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,\n",
       "        0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\n",
       "        1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,\n",
       "        0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\n",
       "       -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\n",
       "       -0.64426 , -0.51042 ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector for the word \"king\"\n",
    "embeddings_dict['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization of tokenized sentences with pretrained GloVe vectors\n",
    "# Input: X - the tokenized sentences\n",
    "#        dic - dictionary of loaded GloVe vectors\n",
    "def vectorize_glove(X, dic):\n",
    "    X_vectorized = []\n",
    "    # TODO\n",
    "\n",
    "    return np.array(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vectorize_glove(X, embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy:  0.8327\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt8TKXqhDcmB"
   },
   "source": [
    "### Part 4: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dajg-I2r5khr"
   },
   "source": [
    "Word2Vec is a popular technique of natural language processing to create word embeddings using neural networks, which can be further used for various text related tasks, e.g. sentiment anlaysis. In this problem, we will walk you through the implementation of Skip-Gram model, and the use of Gensim, a popular pacakge for Word2Vec. \n",
    "<br>   \n",
    "In week 2 of 16ML, you have learned the mechanism of gradient descent as well as the implementation of stochastic gradient descent. In this problem, we will let you implement parts of the train function, including generating input and target data from raw tokenized sentences, and compute losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLl8I8y0xj22"
   },
   "source": [
    "**(a)**\n",
    "The training objective (for one training sample) is to maximize the conditional probability of observing the actual context word $w_{Oj}$ given the input context word $w_I$.\n",
    "\\begin{equation*}\n",
    "\\ P(w_{O,1}, w_{O,2}, ... , w_{O,C}| w_I) = \\prod_{c=1}^C\\frac{e^{u_{j^*_c}}}{\\sum_{j'}^V e^{u_{j'}}} \n",
    "\\end{equation*}\n",
    "Where $u$ is the output layer before softmax, and $u_{j^*_c}$ is the node of $c^{th}$ context word in the output layer.\n",
    "<br> Derive the loss function $E$ by taking the negative log of the above expression, and further prove that $$\\frac{\\partial E}{\\partial u_j} = t_j - y_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91Uzph1NxoNK"
   },
   "source": [
    "**(b)** The softmax function is a function that turns a vector of V real values into a vector of the same length that sums to 1, and each value is between 0 and 1, so that they can be interpreted as probabilities. Implement softmax function in the next cell. \n",
    "\\begin{equation*}\n",
    "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j^V e^{z_j}}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "VtzMhWbdxsEq"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ## TODO: Compute softmax values for each sets of scores in x.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7TiIMcm9qDk"
   },
   "source": [
    "**(c)**\n",
    "The following cell is the implementation of Word2Vec (Skip-Gram model) using stochastic gradient descent. Fill in the TODO parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "VkyqjFMqxxZu"
   },
   "outputs": [],
   "source": [
    "class word2vec(object): \n",
    "    def __init__(self, eta = 0.005, window_size = 2, hidden_num = 10): \n",
    "      # self.N - the number of neurons in the single hidden layer\n",
    "      # self.words - the list of words\n",
    "      # self.eta - the learning rate\n",
    "      # self.vocab - the dictionary which maps the word to its index of self.words\n",
    "      # self.V - the length of the words list\n",
    "      # self.W1 - the first weight matrix of dimension |V| * N\n",
    "      # self.W2 - the second weight matrix of dimension N * |V|\n",
    "      # self.h - the hidden layer\n",
    "      # self.u - the output layer\n",
    "\n",
    "        self.N = hidden_num\n",
    "        self.window_size = window_size\n",
    "        self.eta = eta\n",
    "        self.words = [] \n",
    "        self.vocab = {}\n",
    "        self.V = None\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.h = None\n",
    "        self.u = None\n",
    "        \n",
    "\n",
    "    def initialize(self, X):\n",
    "        # build the vocabulary from the tokenized sentences and update self.V, self.words\n",
    "        data = {} \n",
    "        for tokens in X: \n",
    "            for word in tokens: \n",
    "                if word not in data: \n",
    "                    data[word] = 1\n",
    "                else: \n",
    "                    data[word] += 1\n",
    "                    \n",
    "        self.V = len(data) \n",
    "        self.words = sorted(list(data.keys())) \n",
    "        \n",
    "        for i in range(len(data)): \n",
    "            self.vocab[self.words[i]] = i \n",
    "   \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "\n",
    "\n",
    "    # Implement the forward function, update self.h and self.u, and return the result of softmax classification\n",
    "    # X - the one hot vectors of center words\n",
    "    def forward(self,X): \n",
    "        # TODO: update self.h and self.u, and return the result of softmax classification\n",
    "        pass   \n",
    "  \n",
    "    # Implement the backward function, update self.W1 and self.W2\n",
    "    # input: X - the one hot vector of center word\n",
    "    #        y - the output of softmax classification\n",
    "    #        target - the representation of center word's context words in a single vector\n",
    "    def backward(self,X,y,target): \n",
    "        # TODO: update self.W1 and self.W2\n",
    "        pass \n",
    "           \n",
    "    def train(self,epochs, X):\n",
    "        numTokens = len(sum(X, []))\n",
    "        iterable = range(numTokens)\n",
    "        itbar = tqdm(iterable)\n",
    "        for e in epochs: \n",
    "            itbar.refresh()\n",
    "            itbar.reset()\n",
    "            self.loss = 0\n",
    "            vocab = self.vocab\n",
    "\n",
    "            for sentence in X: \n",
    "                for i in range(len(sentence)): \n",
    "                    itbar.update()\n",
    "                    \n",
    "                    ## TODO: You need to process tokens of sentences (self.X) into valid training points\n",
    "                    # 1. the input x (center_word) is an one hot encoding vector of the length of the vocabulary  \n",
    "                    # 2. the output y (context_word) is of the same length of x, which has 1's in the sliding window of\n",
    "                    #    the center_word except the center_word itself. Be catious of the boundary\n",
    "                    \n",
    "                    # center_word = \n",
    "                    # context = \n",
    "\n",
    "                    y = self.forward(center_word) \n",
    "                    self.backward(center_word, y, context) \n",
    "                    \n",
    "                    ## TODO: compute the loss based on the formula you derived\n",
    "                    self.loss += \n",
    "\n",
    "            print(\"epoch \",e+1, \" loss = \",self.loss) \n",
    "            self.eta *= 1/( (1+self.eta*(e+1)) ) \n",
    "\n",
    "    # return context words of the given center word or None if word isn't in the vocabulary\n",
    "    # Input: word - the center word\n",
    "    #        num - the number of context words to return\n",
    "    def predict(self,word,num): \n",
    "        if word in self.words:\n",
    "            # TODO: \n",
    "            \n",
    "            return top_context_words \n",
    "        else: \n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpYEGerQyqNA"
   },
   "source": [
    "**(d)**\n",
    "Because word2vec takes long time to train, you only use the following experiment to test your implementation. Run the next cell and train a word2vec embedding model using your implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ZuWKDezNyfkt"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce27d44bc8a458cb476790ab0c0bc7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd22f4622ab040f5b13c239de62a147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5651.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  loss =  135494.66520099316\n",
      "epoch  2  loss =  135062.39764454716\n",
      "epoch  3  loss =  134651.52066630614\n",
      "epoch  4  loss =  134259.6232236971\n",
      "epoch  5  loss =  133884.69763718877\n",
      "epoch  6  loss =  133525.048051475\n",
      "epoch  7  loss =  133179.21957216744\n",
      "epoch  8  loss =  132845.9460506078\n",
      "epoch  9  loss =  132524.11352444696\n",
      "epoch  10  loss =  132212.73619763908\n",
      "epoch  11  loss =  131910.94217691696\n",
      "epoch  12  loss =  131617.9667352968\n",
      "epoch  13  loss =  131333.15152082779\n",
      "epoch  14  loss =  131055.94881188047\n",
      "epoch  15  loss =  130785.93060439751\n",
      "epoch  16  loss =  130522.80293225615\n",
      "epoch  17  loss =  130266.42614793855\n",
      "epoch  18  loss =  130016.84129319226\n",
      "epoch  19  loss =  129774.29964318038\n",
      "epoch  20  loss =  129539.28417555042\n",
      "epoch  21  loss =  129312.49550057745\n",
      "epoch  22  loss =  129094.75686746852\n",
      "epoch  23  loss =  128886.80371297408\n",
      "epoch  24  loss =  128689.00450853643\n",
      "epoch  25  loss =  128501.173695603\n",
      "epoch  26  loss =  128322.6283329549\n",
      "epoch  27  loss =  128152.45417883898\n",
      "epoch  28  loss =  127989.78719489038\n",
      "epoch  29  loss =  127833.95663443902\n",
      "epoch  30  loss =  127684.48434789122\n",
      "epoch  31  loss =  127541.01992403998\n",
      "epoch  32  loss =  127403.27880286348\n",
      "epoch  33  loss =  127271.00645550684\n",
      "epoch  34  loss =  127143.96435657097\n",
      "epoch  35  loss =  127021.92703687925\n",
      "epoch  36  loss =  126904.68250859356\n",
      "epoch  37  loss =  126792.032529248\n",
      "epoch  38  loss =  126683.79180366668\n",
      "epoch  39  loss =  126579.78634169411\n",
      "epoch  40  loss =  126479.85145522744\n",
      "epoch  41  loss =  126383.82980113411\n",
      "epoch  42  loss =  126291.56971902287\n",
      "epoch  43  loss =  126202.92397318629\n",
      "epoch  44  loss =  126117.74891024017\n",
      "epoch  45  loss =  126035.90398532986\n",
      "epoch  46  loss =  125957.25158139222\n",
      "epoch  47  loss =  125881.65703866853\n",
      "epoch  48  loss =  125808.98881776174\n",
      "epoch  49  loss =  125739.11873270692\n",
      "epoch  50  loss =  125671.92220607349\n",
      "epoch  51  loss =  125607.27851295141\n",
      "epoch  52  loss =  125545.07099321146\n",
      "epoch  53  loss =  125485.1872208981\n",
      "epoch  54  loss =  125427.51912643618\n",
      "epoch  55  loss =  125371.96307168566\n",
      "epoch  56  loss =  125318.4198805801\n",
      "epoch  57  loss =  125266.79482953927\n",
      "epoch  58  loss =  125216.99760254257\n",
      "epoch  59  loss =  125168.94221595804\n",
      "epoch  60  loss =  125122.54691812514\n",
      "epoch  61  loss =  125077.73406847831\n",
      "epoch  62  loss =  125034.4300006391\n",
      "epoch  63  loss =  124992.56487349459\n",
      "epoch  64  loss =  124952.07251387955\n",
      "epoch  65  loss =  124912.89025403297\n",
      "epoch  66  loss =  124874.9587665365\n",
      "epoch  67  loss =  124838.22189906618\n",
      "epoch  68  loss =  124802.62651082165\n",
      "epoch  69  loss =  124768.12231219708\n",
      "epoch  70  loss =  124734.66170887084\n",
      "epoch  71  loss =  124702.19965122231\n",
      "epoch  72  loss =  124670.69348971728\n",
      "epoch  73  loss =  124640.1028366942\n",
      "epoch  74  loss =  124610.38943480495\n",
      "epoch  75  loss =  124581.51703220363\n",
      "epoch  76  loss =  124553.45126448547\n",
      "epoch  77  loss =  124526.15954324845\n",
      "epoch  78  loss =  124499.61095114687\n",
      "epoch  79  loss =  124473.77614317047\n",
      "epoch  80  loss =  124448.62725392055\n",
      "epoch  81  loss =  124424.13781060619\n",
      "epoch  82  loss =  124400.28265145129\n",
      "epoch  83  loss =  124377.03784924098\n",
      "epoch  84  loss =  124354.38063969463\n",
      "epoch  85  loss =  124332.28935438964\n",
      "epoch  86  loss =  124310.74335794317\n",
      "epoch  87  loss =  124289.72298919916\n",
      "epoch  88  loss =  124269.20950614952\n",
      "epoch  89  loss =  124249.18503434843\n",
      "epoch  90  loss =  124229.63251858974\n",
      "epoch  91  loss =  124210.53567764499\n",
      "epoch  92  loss =  124191.87896182\n",
      "epoch  93  loss =  124173.64751318195\n",
      "epoch  94  loss =  124155.82712825583\n",
      "epoch  95  loss =  124138.40422302224\n",
      "epoch  96  loss =  124121.36580007027\n",
      "epoch  97  loss =  124104.69941775537\n",
      "epoch  98  loss =  124088.3931612314\n",
      "epoch  99  loss =  124072.43561522414\n",
      "epoch  100  loss =  124056.81583844336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_w2v = X[:50]\n",
    "w2v = word2vec() \n",
    "w2v.initialize(X_w2v)\n",
    "\n",
    "# train 100 epochs, you should see your loss is contantly decreasing\n",
    "epochs = tqdm(range(100))\n",
    "w2v.train(epochs,X_w2v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VkKpG2X0Ajb"
   },
   "source": [
    "Check your context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "7WWnJuxn0A1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'time', 'like']\n"
     ]
    }
   ],
   "source": [
    "# check your context words\n",
    "print(w2v.predict(\"good\",3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL5pnmxm08An"
   },
   "source": [
    "**(e)**\n",
    "`Gensim` is a useful library to train your word2vec model faster [(check documentation here)](https://radimrehurek.com/gensim/models/word2vec.html). Run the following cell to train your Word2Vec model using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "y2B35y0C1UoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6.5343231e-01,  3.3343163e-01,  1.0844829e+00,  1.3008151e-02,\n",
       "       -2.1023450e+00, -2.4775735e-01, -6.8525428e-01,  1.3870639e-02,\n",
       "        2.5386517e+00, -1.0654656e+00,  1.5140635e+00,  4.0039697e-01,\n",
       "       -1.5814729e+00,  8.6096770e-01,  6.6265388e-04,  2.4283192e-01,\n",
       "        1.3176322e+00,  8.7724930e-01, -3.4980454e-02,  1.1874870e-01,\n",
       "        3.0199044e+00,  1.4826660e+00, -3.1966373e-02,  1.1176403e+00,\n",
       "       -1.5124489e-01,  2.6175010e-01, -1.7535266e+00, -6.0057050e-01,\n",
       "       -6.2212244e-02,  3.7142479e-01,  1.1426917e+00,  5.4532606e-02,\n",
       "        5.6008708e-01, -1.6766489e+00, -7.9946041e-01,  1.1819972e+00,\n",
       "       -1.2723805e+00, -1.7323093e+00, -4.7983339e-01, -1.7789723e+00,\n",
       "        3.5571390e-01, -2.4279227e+00,  9.6058208e-01, -2.2283223e-01,\n",
       "       -1.0313032e+00, -1.8847151e+00, -7.2755539e-01,  6.9904929e-01,\n",
       "        8.5364980e-01,  2.0421745e-01, -9.5761263e-01,  8.9123511e-01,\n",
       "        6.8906076e-02,  6.7103386e-01, -3.2836062e-01, -4.0234071e-01,\n",
       "        1.9253027e+00, -2.6305442e+00,  2.1737809e+00, -1.2124486e+00,\n",
       "        1.6765046e+00,  1.4757158e+00,  6.3910973e-01,  1.3962600e+00,\n",
       "        1.9355226e-01,  3.0620582e+00, -1.8615751e-01,  4.5539445e-01,\n",
       "       -2.2765210e-01,  1.9353383e+00,  6.0986686e-02,  4.0059056e-02,\n",
       "       -9.8370278e-01,  1.1253517e+00,  1.4358042e+00,  9.8901421e-01,\n",
       "       -8.5241991e-01, -5.6580168e-01,  1.0076112e+00,  5.8082062e-01,\n",
       "        1.7582551e-01, -5.8821160e-01,  1.8992780e+00,  5.8919197e-01,\n",
       "       -1.0722052e+00,  1.4127277e-01,  5.8201771e-02,  3.0191811e-02,\n",
       "        5.8144319e-01, -2.3182540e+00, -3.2223517e-01,  1.6842830e+00,\n",
       "       -3.2980380e+00,  3.6046913e-01,  1.3231281e+00, -5.3809588e-03,\n",
       "       -1.5934287e-01, -4.9040839e-01, -5.8875442e-01,  8.0917549e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(X)\n",
    "\n",
    "# model['word'] will give the embedding of the word \"word\"\n",
    "model['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p6ECJS51bFg"
   },
   "source": [
    "**(f)**\n",
    "After you finish training your word2vec model with gensim, vectorize your tokenized sentences. For each sentence as a data point, we simply average the embedding of each word in this sentence. Implement the vectorize_word2vec function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv1YVDqm1Za-"
   },
   "outputs": [],
   "source": [
    "# Use trained word2vec model to vectorize the sentences. \n",
    "# Input: X - the tokenized sentences\n",
    "#        model - the trained word2vec model\n",
    "# return X_vectorized of dimension N * dim_embedding\n",
    "def vectorize_word2vec(X, model):\n",
    "    X_vectorized = []\n",
    "    ## TODO\n",
    "    \n",
    "    return X_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "nv1YVDqm1Za-"
   },
   "outputs": [],
   "source": [
    "X_vectorized = vectorize_word2vec(X, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pofxb9S1YFGV"
   },
   "source": [
    "Check the performance on the task of movie sentiment analysis with word2vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "vCXQJ6ykYEZB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy:  0.857\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2)\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9w_vapDDiBm"
   },
   "source": [
    "### **Part 5: BERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDWzXc-QB_oq"
   },
   "source": [
    "In the last section, we will introduce a state-of-the-art word embedding model called BERT (Bidirectional Encoder Representations from Transformers). It was created and published in 2018 by Jacob Devlin and his colleagues from Google. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks. If you are interested in the details of the Bert model, you can refer to their paper: https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use pre-trained BertWordPieceTokenizer to preprocess the reviews and the pre_trained TFDistilBertModel to classify the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained tokenizer\n",
    "tokenizer = BertWordPieceTokenizer('Data/vocab.txt', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding()\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "        \n",
    "    # Padding the last chunk\n",
    "    res = np.zeros((len(all_ids), maxlen))\n",
    "    for x in range(len(all_ids)):\n",
    "        for y in range(len(all_ids[x])):\n",
    "            res[x][y] = all_ids[x][y]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pre_trained tokenizer on our dataset\n",
    "X_raw_5000 = X_raw[:5000]\n",
    "y_5000 = y[:5000]\n",
    "X_tokenized = fast_encode(X_raw_5000.values, tokenizer, maxlen=400)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tokenized, y_5000, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=400):  \n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(name='Movie_Reviews_Classification',inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Movie_Reviews_Classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 400)]             0         \n",
      "_________________________________________________________________\n",
      "tf_distil_bert_model (TFDist ((None, 400, 768),)       66362880  \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 66,363,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the pre_trained tokenizer\n",
    "bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# Customize the pre_trained model to fullfill our task\n",
    "model = build_model(bert_model, max_len=400)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After intializing the model, we need to apply it on the movie reviews dataset. Since the training process takes hours to run with CPU, we have trained the result for you. You can skip the next cell and load the results in the following cell directly. If you want to experiment with the training process, feel free to uncomment the code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line takes hours to run, so don't uncomment it unless you want to experiment the training process.\n",
    "# model.fit(X_train,y_train,batch_size = 32 ,validation_data=(X_test,y_test),epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/32 [=======>......................] - ETA: 2:24 - loss: 0.2353 - accuracy: 0.9201"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-281df907a176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy of the model on Testing Data is - \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_weights(\"Data/bert_model\")\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.load('my_history.npy',allow_pickle='TRUE').item()\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "\n",
    "axes[0].plot(history['accuracy'])\n",
    "axes[0].plot(history['val_accuracy'])\n",
    "axes[0].set_title('model accuracy')\n",
    "axes[0].set_ylabel('accuracy')\n",
    "axes[0].set_xlabel('epoch')\n",
    "\n",
    "\n",
    "axes[1].plot(history['loss'])\n",
    "axes[1].plot(history['val_loss'])\n",
    "axes[1].set_title('model loss')\n",
    "axes[1].set_ylabel('loss')\n",
    "axes[1].set_xlabel('epoch')\n",
    "\n",
    "fig.legend(['train', 'val'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Comment on the result of Bert model and compare it with the previous models.\n",
    "\n",
    "(b) Comment on the graph of model accuracy and loss. What do you think will happen if we run the model with more ephochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start answering part5\n",
    "\n",
    "#### End answering part5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EECS289_Final_Project_T.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
