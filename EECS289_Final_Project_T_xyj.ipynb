{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1w6S2tf_1r4"
   },
   "source": [
    "# **EECS 16ML Assignment: Traditional/Contextual Word Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE0fNvxdAfxt"
   },
   "source": [
    "Before you start, make sure you review the lecture slide and notes about word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGllTplLAP5V",
    "outputId": "e9ec7272-b594-48a4-9d03-3168361cd8b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cathyxu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# if error appears, uncomment the next line\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OM4Mwv-cXoEW"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(review):\n",
    "    # remove HTML contents.\n",
    "    soup = BeautifulSoup(review, \"html.parser\")\n",
    "    review = soup.get_text()\n",
    "\n",
    "    # remove everything except lower/upper case letters using Regular Expressions.\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "\n",
    "  # bring everything into lowercase.\n",
    "    review = review.lower()\n",
    "\n",
    "    review = review.split()\n",
    "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "vkPT8RaPHKT2",
    "outputId": "6c46b625-c024-4790-e485-7f2f8b8c5b5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the dataset and visualize the first ten data points\n",
    "path_data = 'IMDB Dataset.csv'\n",
    "imdb_data=pd.read_csv(path_data)\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "LXA3S5ocKsPn",
    "outputId": "7ed21d38-d3d8-4f1a-c773-6de84219641f"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and testing dataset\n",
    "X_raw = imdb_data['review'][:5000]\n",
    "y = imdb_data['sentiment'][:5000]\n",
    "\n",
    "#tqdm.pandas()\n",
    "#X = X_raw.progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGIb63I8EWq4"
   },
   "source": [
    "### **Part 1: Word-Count Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF-BW6TOQ9mm"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "  Given several pieces of text, create a list of unique word tokens included in all the texts\n",
    "  \"\"\"\n",
    "    return\n",
    "\n",
    "def vectorize(tokens):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSRpvE80EkPJ"
   },
   "source": [
    "### **Part 2: TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAjiXPMXKKzO"
   },
   "outputs": [],
   "source": [
    "# This function computes the term frequency (TF), i.e. the number of times a word appears in a review divded \n",
    "# by the total number of words in the review.\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "# Output: tfDict - a dictionary containing the TF scores of all the words in a review\n",
    "def computeTF(bagOfWords):\n",
    "    # Frequency of each word in a review\n",
    "    wordDict = dict(Counter(bagOfWords))\n",
    "    tfDict = {}\n",
    "    N = len(bagOfWords)\n",
    "  # TODO: compute term frequency\n",
    "  # Hint: Don't forget to apply logarithm transformation \n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = np.log(1+count / float(N))\n",
    "    return tfDict\n",
    "\n",
    "# This function computes the inverse document frequency (IDF), i.e. the number of reviews divided by the number \n",
    "# of reviews that contain the word w. \n",
    "# Input: reviews - a list containing all the reviews\n",
    "# Output: idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "def computeIDF(reviews):\n",
    "    N = len(reviews)\n",
    "    reviews = [dict(Counter(review)) for review in reviews]\n",
    "    idfDict = dict.fromkeys(set().union(*(review.keys() for review in reviews)),0)\n",
    "        \n",
    "  # TODO: compute inverse document frequency\n",
    "  # Hint: Don't forget to apply logarithm transformation \n",
    "\n",
    "    for document in reviews:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = np.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "# This function computes the TF-IDF score of all the words in a review\n",
    "# Input: bagOfWords - a list containing all the words in a review\n",
    "#        idfDict - a dictionary containing the IDF score of all the words in all the reviews\n",
    "# Output: tfidf - a dictionary containing the TF-IDF scores of all the words in a review\n",
    "def computeTFIDF(bagOfWords, idfDict):\n",
    "    tfidf = {}  \n",
    "    tfDict = computeTF(bagOfWords)\n",
    "  # TODO: compute tf-idf\n",
    "    for word, val in tfDict.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "  \n",
    "    return tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF(X)\n",
    "total_vocab = list(idfs.keys())\n",
    "tfidf = X.progress_apply(computeTFIDF, idfDict=idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized(tfidf_doc):\n",
    "    vec = np.zeros(len(total_vocab))\n",
    "    for word, val in tfidf_doc.items():\n",
    "        ind = total_vocab.index(word)\n",
    "        vec[ind] = val\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = tfidf.progress_apply(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(X_vectorized), y, test_size = 0.2,\n",
    "                                                    random_state = 0)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv=TfidfVectorizer()\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(X_raw)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tv_train_reviews, y, test_size = 0.2,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt8TKXqhDcmB"
   },
   "source": [
    "###**Part 3: Word2vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9w_vapDDiBm"
   },
   "source": [
    "**Beyond Word2vec: Capturing Word Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDWzXc-QB_oq"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# Load vectors directly from the file\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_word2vec(X):\n",
    "    count = 0\n",
    "    X_vectorized = []\n",
    "    for i in range(len(X)):\n",
    "        X_words = []\n",
    "        for word in X[i]:\n",
    "            if word in model:\n",
    "                X_words.append(model[word])\n",
    "                \n",
    "            else:\n",
    "                X_words.append(np.zeros(300)) \n",
    "                count += 1\n",
    "        X_vectorized.append(np.sum(np.array(X_words), axis = 0).reshape(-1))\n",
    "    print(count)\n",
    "    return np.array(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vectorized = vectorize_word2vec(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size = 0.2,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = MLPClassifier(alpha=1e-5, max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as ppb\n",
    "\n",
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "model.eval()\n",
    "\n",
    "tokenized = X_raw.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 512)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 512)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_ids, attn_masks):\n",
    "        assert(len(input_ids) == len(attn_masks))\n",
    "        self.input_ids = input_ids\n",
    "        self.attn_masks = attn_masks\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.input_ids[index]), torch.tensor(self.attn_masks[index]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_dataset = BertDataset(padded, attention_mask)\n",
    "train_bert_dataloader = DataLoader(train_bert_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "10 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "20 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "30 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "40 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "50 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "60 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "70 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "80 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "90 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "100 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "110 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "120 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "130 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "140 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "150 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "160 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "170 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "180 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "190 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "200 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "210 torch.Size([8, 512]) torch.Size([8, 512])\n",
      "220 torch.Size([8, 512]) torch.Size([8, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d2e8b64587ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1698\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_hidden_states = []\n",
    "with torch.no_grad():\n",
    "    for i, (input_ids, attn_mask) in enumerate(train_bert_dataloader):\n",
    "        if i % 10 == 0:\n",
    "            print(i,input_ids.shape, attn_mask.shape)\n",
    "        last_hidden_states.append(model(input_ids, attention_mask=attn_mask, return_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][0][:,0,:].numpy()\n",
    "for i in range(1,len(last_hidden_states)):\n",
    "    features = np.concatenate((features, last_hidden_states[i][0][:,0,:].numpy()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size = 0.2,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy:  0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes = (100,100),max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('\\n Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching oz episode ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little filming technique fashion giv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer si...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy thinks zombie clos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter time visually stunning film mattei offe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one reviewers mentioned watching oz episode ex...          1\n",
       "1  wonderful little filming technique fashion giv...          1\n",
       "2  thought wonderful way spend time hot summer si...          1\n",
       "3  basically family little boy thinks zombie clos...          0\n",
       "4  petter time visually stunning film mattei offe...          1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re,string,unicodedata\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
    "            final_text.append(i.strip().lower())\n",
    "    return \" \".join(final_text)\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(denoise_text)\n",
    "\n",
    "imdb_data.sentiment.replace(\"positive\" , 1 , inplace = True)\n",
    "imdb_data.sentiment.replace(\"negative\" , 0 , inplace = True)\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(imdb_data.review[:5000],imdb_data.sentiment[:5000],random_state = 0 , stratify = imdb_data.sentiment[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras import backend as K\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
    "from keras.models import Model\n",
    "import torch\n",
    "import transformers\n",
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n",
    "\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding()\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        \n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "\n",
    "    res = np.zeros((len(all_ids), maxlen))\n",
    "    for x in range(len(all_ids)):\n",
    "        for y in range(len(all_ids[x])):\n",
    "            res[x][y] = all_ids[x][y]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fast_encode(x_train.values, fast_tokenizer, maxlen=400)\n",
    "x_test = fast_encode(x_test.values, fast_tokenizer, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=400):\n",
    "    \n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 400)]             0         \n",
      "_________________________________________________________________\n",
      "tf_distil_bert_model (TFDist ((None, 400, 768),)       66362880  \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 66,363,649\n",
      "Trainable params: 66,363,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_model, max_len=400)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 2752s 23s/step - loss: 0.6317 - accuracy: 0.6096 - val_loss: 0.4796 - val_accuracy: 0.7784\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 3102s 26s/step - loss: 0.3661 - accuracy: 0.8427 - val_loss: 0.3306 - val_accuracy: 0.8440\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 3032s 26s/step - loss: 0.2333 - accuracy: 0.9099 - val_loss: 0.3545 - val_accuracy: 0.8544\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,batch_size = 32 ,validation_data=(x_test,np.array(y_test)),epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 242s 6s/step - loss: 0.3545 - accuracy: 0.8544\n",
      "Accuracy of the model on Testing Data is -  85.43999791145325 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-05be3afa581b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"bert_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 240s 6s/step - loss: 0.3545 - accuracy: 0.8544\n",
      "Accuracy of the model on Testing Data is -  0.8543999791145325 %\n"
     ]
    }
   ],
   "source": [
    "a = build_model(bert_model, max_len=400)\n",
    "a.load_weights(\"bert_model\")\n",
    "print(\"Accuracy of the model on Testing Data is - \" , a.evaluate(x_test,y_test)[1] , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=np.load('my_history.npy',allow_pickle='TRUE').item()\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6316890716552734, 0.36613354086875916, 0.23334290087223053],\n",
       " 'accuracy': [0.6096000075340271, 0.8426666855812073, 0.9098666906356812],\n",
       " 'val_loss': [0.47957319021224976, 0.33058369159698486, 0.3545358180999756],\n",
       " 'val_accuracy': [0.7784000039100647, 0.843999981880188, 0.8543999791145325]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EECS289_Final_Project_T.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
